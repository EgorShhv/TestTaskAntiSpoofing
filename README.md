# TestTaskAntiSpoofing

**Цель работы:** обучить модель классификации для задачи face antispoofing, получить максимальные метрики на тестовых данных. В итоге модель должна разделять изображения на **два класса** – реальное лицо и спуффинг. Решение должно быть залито на github. Если решение не получается запустить и воспроизвести, оно не будет учитываться.

**Набор данных:** zip-архив. Внутри две директории - train(содержит 5000 фото) и test(содержит 968 фото). Также в архиве имеется две таблицы - train.csv и test.csv, которые относятся к одноименным директориям. Таблицы имеют 4 колонки:
| image | spoof_type | spoof_label | box |
| --- | --- | --- | --- |
| Путь до изображения | Тип спуфинга | Бинарная метка | Бокс лица в формате [x_min, y_min, x_max, y_max] |

**Использованные инструменты:** 

язык программирования - python 3.8.10, 

фреймворк машинного обучения - PyTorch 1.13.1+cu116,

логирование - wandb 0.13.9 


---


[1. Ход эксперимента](#Ход)


[2. Логи эксперимента](#Логи)


[3. Демонстрация работы](#Демо)


---


<a name="Ход"></a>
## 1. Ход эксперимента

Ход эксперимента можно увидеть в файле _experiment.ipnb_

Входной набор данных для тестов состоит из **3359** изображений с меткой Spoof и **1641** с меткоц Live.

Данные являются несбалансированными для задачи разделения на 2 класса, поэтому имеет смысл попытаться расширить данные.

Для аугментации данных будем использовать простой метод отзеркаливания по горизонтали. Т.к. поддельных изображений в два раза меньше реальных, можно отзеркалить каждое реальное изображение один раз. При этом важно получить новый бокс для лица. Для вычисления новых координат бокса координаты по Y останутся неизменными, а новые координаты по оси X вычислятся по формулам: 

x_**min**\_new = img_x_max - x_**max**\_old

x_**max**\_new = img_x_max - x_**min**\_old

Имея исоходный набор и аугментированный, а также бокс лица для каждого изображения составим 4 плана эксперимента:
1. Исходный набор
2. Исходный набор с обрезанными по боксу изображениями
3. Аугментированный набор
4. Аугментированный набор с обрезанным по боксу лицом

В качестве моделей воспользуемся предобученными моделями из модуля torchvision.Возьмем несколько моделей, отличающиеся архитектурой и сложностью:
1. AlexNet
2. EfficientNet
3. GoogLeNet
4. ResNet

Параметры для всех моделей установленые для первых экспериментов были следующими:

Количество эпох: 5,

batch_size: 35,

learning_rate: 1e-3,

Оптимизатор: Adam

Критерий оптимальности: Кросс энтропия

Критерий качества: Accuracy

<a name="Логи"></a>
## 2. Предварительные реузльтаты и логи эксперимента

Для логирования эксперимента использовался сервис Weights & Biases.

Логи можно посмотреть по ссылке: https://wandb.ai/egshhv/Test_task?workspace=user-egshhv

В ходе работы наилучший результат показала модель EfficientNet для аугментированного датасета с использованием обрезки по боксу. Качество на тестовой выборке составило 0.9867.

Матрица ошибок на тестовом наборе для модели представлена на рисунке ниже:

![image](https://user-images.githubusercontent.com/92858334/214109047-9aaef3ea-5730-404a-9316-af934935987c.png)

Для задачи антиспуфинга предпочтительнее минимизировать ошибку для поддельных изображений. В тестовом наборе 68 изображений с меткой Spoof и модель ошиблась в 6 из них.

Проблема состоит в том, что необходимо иметь бокс лица для каждого изображения. Если смотреть эксперименты без обрезки по боксу, то наилучшее качество на тестовой выборке показала та же сеть EfficientNet, но для базового набора. Точность = 0.9709. Но целых 20 поддельных изображений было ошибочно определено как реальные.

Матрица ошибок на тестовом наборе для модели представлена на рисунке ниже:

![image](https://user-images.githubusercontent.com/92858334/214110828-0ed87147-a235-4acb-bd4f-e205e35e2918.png)

При этом модель EfficientNet для аугментированного набора без обрезки по боксу лица имеет точность 0.9694, но ошибки в поддельных изображениях в тестовом наборе составляют 15 из 68.

Матрица ошибок на тестовом наборе для модели представлена на рисунке ниже:

![image](https://user-images.githubusercontent.com/92858334/214111371-8a782ce8-69b4-4760-8513-fa6f1e5afe2b.png)


<a name="Демо"></a>
## 3. Демонстрация работы

Файл _efficientnet_aug_data_croppedJIT.pth_ хранит модель PyTorch, которая обучалась на данных с обрезкой по боксу, а файл _efficientnet_aug_dataJIT.pth_ - модель, которая обучалась на данных без обрезки. Для демонстрации работы был написан скрипт на языке Python.

Для запуска необходимо иметь интерпритатор Python, а также установленные библиотеки torch, torchvision и skimage.

Чтобы запустить скрипт необходимо выполнить команду 
```no-highlight
python test_script.py img_path model_path
```

где _img_path_ - путь до изображения, которое необходимо классифицировать, _model_path_ - путь до используемой модели.

Для тестов возьмем 4 изображения - два поддельных и два реальных.

![image](https://user-images.githubusercontent.com/92858334/214140565-f8e40082-a224-45d9-9ce5-015916bd2a9b.png)
![image](https://user-images.githubusercontent.com/92858334/214140681-c87ce5e6-8268-4aaa-ad2e-1f452c8de074.png)
![image](https://user-images.githubusercontent.com/92858334/214140717-af90c7a9-7c65-4014-9e92-519745c176a0.png)
![image](https://user-images.githubusercontent.com/92858334/214140766-72c29e17-dad2-47ca-96d9-feee7a8f062a.png)

Верный порядок: Spoof, Spoof, Live, Live.

Модель, которая обучалась на данных без обрезки по боксу, выдала для изображений выше следующие результаты: 
Spoof, Spoof, Live, Live.

В модель, которая обучалась на датасете с обрезкой по боксу, передавались обрезанные по лицу изображения. Результатом был следующий: 
Spoof, Spoof, Live, Live.

Пример запуска представлен на скриншоте ниже. Запуск осуществлялся в Google Colab

![image](https://user-images.githubusercontent.com/92858334/214145095-92859dc2-597c-48e7-8bd6-6f243f6988e4.png)






